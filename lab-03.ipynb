{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c99f4ed-d6e8-4f0c-bdd4-bf6c1662a39f",
   "metadata": {},
   "source": [
    "# Lab 03: Text Classification on the DBpedia14 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e74017-06ac-4e30-afe2-438f413d2a95",
   "metadata": {},
   "source": [
    "### Objectives:\n",
    "1. Build a Naive Bayes classification model from scratch\n",
    "2. Evaluate the performance of your model on the DBpedia14 dataset\n",
    "3. Train an off-the-shelf NB classifier and compare its performance to your implementation\n",
    "4. Train off-the-shelf implementations of the linear-SVM, RBF-kernel-SVM, and perceptron and compare their performance with the NB models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad5be2-5882-4cb6-95cb-18641199b02b",
   "metadata": {},
   "source": [
    "### Suggested Reading\n",
    "\n",
    "1. https://arxiv.org/pdf/1811.12808.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db7b42-9a27-45bd-b0e4-77106357c9d6",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a83cd2ac-a840-4d36-b22d-3b4215cb99d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset dbpedia_14 (/Users/jieyisun/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n",
      "100%|██████████| 2/2 [00:00<00:00, 255.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "train_ds, test_ds = datasets.load_dataset('dbpedia_14', split=['train', 'test'])\n",
    "df_train: pd.DataFrame = train_ds.to_pandas().sample(frac = 0.1).reset_index(drop=True)\n",
    "df_test: pd.DataFrame = test_ds.to_pandas().sample(frac = 0.1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d89846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56000, 7000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52be4ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>Scroogenomics</td>\n",
       "      <td>Scroogenomics is a non-fiction book written b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>John McGee (politician)</td>\n",
       "      <td>John McGee (born January 27 1973) is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>Spiral Ascent</td>\n",
       "      <td>Spiral Ascent is the debut album by Indian He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>Lost Horizon (1973 film)</td>\n",
       "      <td>Lost Horizon is a 1973 American musical film ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Swift River (New Zealand)</td>\n",
       "      <td>The Swift River is a river of the Canterbury ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                      title  \\\n",
       "0     13              Scroogenomics   \n",
       "1      4    John McGee (politician)   \n",
       "2     11              Spiral Ascent   \n",
       "3     12   Lost Horizon (1973 film)   \n",
       "4      7  Swift River (New Zealand)   \n",
       "\n",
       "                                             content  \n",
       "0   Scroogenomics is a non-fiction book written b...  \n",
       "1   John McGee (born January 27 1973) is a former...  \n",
       "2   Spiral Ascent is the debut album by Indian He...  \n",
       "3   Lost Horizon is a 1973 American musical film ...  \n",
       "4   The Swift River is a river of the Canterbury ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0086be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56000 entries, 0 to 55999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    56000 non-null  int64 \n",
      " 1   title    56000 non-null  object\n",
      " 2   content  56000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27bb6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = df_train[\"label\"]\n",
    "max(label) #13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d5f1c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56000</td>\n",
       "      <td>56000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>56000</td>\n",
       "      <td>55999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Lar Corbett</td>\n",
       "      <td>Alturas Lake is an alpine lake in Blaine Coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title                                            content\n",
       "count         56000                                              56000\n",
       "unique        56000                                              55999\n",
       "top     Lar Corbett   Alturas Lake is an alpine lake in Blaine Coun...\n",
       "freq              1                                                  2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a791197-515c-4dce-aac8-e8a4ba40be10",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part I: Build your own Naive Bayes classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2542f-9461-427d-93bb-1d39b1627afe",
   "metadata": {},
   "source": [
    "### (10 pts) Task I: Build a model from scratch\n",
    "Using your notes from lecture-02, implement a Naive Bayes model and train it on the DBpedia dataset. Also, feel free to use any text preprocessing you wish, such as the pipeline from Lab02. \n",
    "\n",
    "Below is a template class to help you think about the structure of this problem (feel free to design your own code if you like). It contains methods for each inference step in NB. It also has a classmethod that you could use to instantiate the class from a list of documents and a corresponding list of labels. Here we are suggesting you create a dictionary that maps each word to a unique $ith$ index in the $\\phi_{i,k}$ probabilty matrix, which you need to estimate. Because the labels are a set of 0-indexed integers, they naturally map to a unique position $\\mu_{k}$ (you should check this to make sure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45d83a42-f957-467a-a101-8be81751f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jieyisun/opt/miniconda3/envs/TF/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "from cProfile import label\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class NaiveBayesModel:\n",
    "    \n",
    "    \"\"\"Multinomial NB model class template\"\"\"\n",
    "    \n",
    "    phi: np.ndarray # (N, K)\n",
    "    \n",
    "    mu: np.ndarray  # (K,)\n",
    "    \n",
    "    vocab: dict     # vocabulary map from word to row index in phi\n",
    "    \n",
    "    n_class: int    # number of classes   \n",
    "    \n",
    "    DF_Count_T: pd.core.frame.DataFrame   # word count of docs\n",
    "    \n",
    "    labels_list: List[int]\n",
    "    \n",
    "    \n",
    "    def __init__(self, vocabulary: dict, num_classes: int, DF_Count_T: pd.core.frame.DataFrame, labels_list: List[int]):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocabulary: {str: int} <- {word: index}\n",
    "        num_classes: Number of classes\n",
    "        \"\"\"\n",
    "        self.vocab = vocabulary\n",
    "        self.n_class = num_classes\n",
    "        self.mu = np.zeros(shape = (num_classes,))\n",
    "        N = len(vocabulary.keys())\n",
    "        self.phi = np.zeros(shape = (N, num_classes))\n",
    "        self.labels_list = labels_list\n",
    "        self.DF_Count_T = DF_Count_T\n",
    "    \n",
    "    @classmethod\n",
    "    def from_preprocessed_data(cls, docs_list: List[str], labels_list: List[int]):\n",
    "        # extracting features from text using countvectorizer\n",
    "        # create a vectorizer object\n",
    "        cv = CountVectorizer(input='content', stop_words = \"english\", max_features=400)\n",
    "        # tokenize and build vocab, encode document\n",
    "        X = cv.fit_transform(docs_list)\n",
    "        #print(cv.vocabulary_)\n",
    "\n",
    "        ColumnNames = cv.get_feature_names()\n",
    "        DF_Count_T = pd.DataFrame(X.toarray(),columns=ColumnNames).T\n",
    "        DF_Count = DF_Count_T.T\n",
    "        \n",
    "        labels_set = set(labels_list)\n",
    "        num_classes = len(labels_set)\n",
    "\n",
    "        vocabulary = dict()\n",
    "        for i in range(len(DF_Count_T.index)):\n",
    "            vocabulary[DF_Count.columns[i]] = i\n",
    "        return cls(vocabulary, num_classes, DF_Count_T, labels_list)\n",
    "\n",
    "    \n",
    "    def estimate_mu(self, alpha: float = 1.):\n",
    "        \"\"\"\n",
    "        Estimate P(Y), the prior over labels\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: smoothing parameter\n",
    "        \"\"\"\n",
    "        for i in self.labels_list:\n",
    "            self.mu[i] += 1\n",
    "        self.mu = (self.mu + alpha) / (sum(self.mu) + self.n_class * alpha)\n",
    "        return self.mu\n",
    "    \n",
    "    def estimate_phi(self, alpha: float = 1.):\n",
    "        \"\"\"\n",
    "        Estimate phi, the N x K matrix \n",
    "        describing the probability of\n",
    "        the nth word in the kth class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: smoothing parameter\n",
    "        \"\"\"\n",
    "        #create an empty matrix\n",
    "        NK = []\n",
    "        for i in range(self.n_class):\n",
    "            NK.append([])\n",
    "        \n",
    "        for i in range(len(self.labels_list)):\n",
    "            NK[self.labels_list[i]] += [i]\n",
    "        \n",
    "            \n",
    "        for i in range(self.n_class):\n",
    "            k = self.DF_Count_T.iloc[:,NK[i]]\n",
    "            #the word count of all words in class k\n",
    "            k_sum = k.apply(lambda x: x.sum(), axis=1)\n",
    "            for word in k_sum.index:\n",
    "                self.phi[self.vocab[word],i] = k_sum[self.vocab[word]]\n",
    "        \n",
    "        self.phi = np.array(pd.DataFrame(self.phi).apply(lambda x: (x+alpha)/(x.sum()+len(x)*alpha), axis=0))\n",
    "        \n",
    "        return self.phi\n",
    "    \n",
    "    def predict_label(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Compute label given some input text\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: raw input text\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int: corresponding to the predicted label\n",
    "        \"\"\"\n",
    "        text = text.split(\" \")\n",
    "        value = list(self.mu)\n",
    "        for word in text:\n",
    "            if word in self.vocab.keys():\n",
    "                for j in range(self.n_class):\n",
    "                    value[j] = value[j] * self.phi[self.vocab[word],j]\n",
    "        return value.index(max(value))\n",
    "\n",
    "model = NaiveBayesModel.from_preprocessed_data(df_train[\"content\"], df_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946846ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NK = [] #save the result in a N x K matrix\n",
    "\n",
    "N = [1,1,1,1]\n",
    "K = [1,1,1]\n",
    "#create an empty N x K matrix\n",
    "for i in range(len(N)):\n",
    "    NK.append([])\n",
    "    for j in range(len(K)):\n",
    "        NK[i].append(0)\n",
    "\n",
    "NK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b4ff39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07246403, 0.07035741, 0.07305317, 0.07339237, 0.071982  ,\n",
       "       0.07094655, 0.06841147, 0.07119649, 0.07276752, 0.07173207,\n",
       "       0.06991109, 0.07060735, 0.07121434, 0.07196415])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = model.estimate_mu()\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "127f9084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0015015 , 0.00143091, 0.00389731, ..., 0.00312975, 0.00073335,\n",
       "        0.00119646],\n",
       "       [0.00080636, 0.00269237, 0.00362937, ..., 0.00201846, 0.00052382,\n",
       "        0.0010877 ],\n",
       "       [0.00108442, 0.00788884, 0.00302041, ..., 0.00272152, 0.0008905 ,\n",
       "        0.00125085],\n",
       "       ...,\n",
       "       [0.00606162, 0.00363376, 0.00604082, ..., 0.00129272, 0.002331  ,\n",
       "        0.00372536],\n",
       "       [0.00130686, 0.00054601, 0.00211916, ..., 0.00124736, 0.00440009,\n",
       "        0.00397009],\n",
       "       [0.00144589, 0.00062132, 0.00129098, ..., 0.00099789, 0.0002881 ,\n",
       "        0.00043508]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = model.estimate_phi()\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa09f00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 12,  5, ...,  7,  5,  3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_Result = []\n",
    "for doc in df_test['content']:\n",
    "    NB_Result.append(model.predict_label(doc))\n",
    "NB_Result = np.array(NB_Result)\n",
    "NB_Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3354bec",
   "metadata": {},
   "source": [
    "# Part II: Model performance evaluation\n",
    "\n",
    "Evaluating the performance of a classification model may seem as simple as computing an accuracy, and in some cases that is sufficient, but in general accuracy is not a reliable metric by itself. Typically we need to evaluate our model using several different metrics. \n",
    "\n",
    "One common issue is class imbalance, which is when the label distribution in the data varies far from uniform. In this case a high accuracy can be misleading because low frequency labels don't contribute equally to the score. More generally, this is one of the biggest drawbacks of using MLE in NLP: models tend to be much less sensitive to low probability labels than to higher probabilty labels. Later in this class we will explore models that learn by predicting words given their context, can you think of reasons why this can be problematic? Hint: remember Zipf's law?\n",
    "\n",
    "Another reason to use multiple evaluation methods is that it can help you better understand your data. Evaluating performance on individual classes often reveals problems with the data that would otherwise go unnoticed. For example, if you observe an abundance of misclassified data specific to only a few classes, chances are you have inconsistent labels for those classes in the training set. This is very common in 3rd party mechanical turk data, where quality can vary wildly.\n",
    "\n",
    "In this lab we will use three metrics and one visualization tool:\n",
    "\n",
    "1. [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)\n",
    "2. [F1 score](https://en.wikipedia.org/wiki/F-score)\n",
    "3. [AUC ROC score](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "4. [The confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "The [metrics module](https://scikit-learn.org/stable/modules/model_evaluation.html) within sklearn provides support for nearly any evaluation metric that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e193424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0f94d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "Acc = accuracy_score(NB_Result, df_test['label'])\n",
    "print(Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b105c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8503527856550249\n"
     ]
    }
   ],
   "source": [
    "# F1 Score\n",
    "F1 = f1_score(NB_Result, df_test['label'], average='weighted')\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89044f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[367   6  11  11   5   9  16   6   0   1   5  10   5  13]\n",
      " [ 22 465   4   4   0   0  11   2   1   0   0   0   1   3]\n",
      " [ 17   1 306  60  20   2   3   0   1   2   2  49  16  33]\n",
      " [  5   0  12 457  11   2   0   3   0   0   0   6   0   2]\n",
      " [  6  12  17  54 360   5   3   0   3   1   5   1   0   2]\n",
      " [ 30   0   4  24   6 406   8   7   1   0   1   7  10   4]\n",
      " [ 27  15   9  16   3  14 411  12   3   3   0   1   3   5]\n",
      " [  4   0   0   3   0   3  11 449   7   3   0   0   0   0]\n",
      " [  0   3   0   0   0   0   4  10 507   0   0   0   0   0]\n",
      " [  4   1   1  14   2   0   1   6   0 372  67   0   0   1]\n",
      " [  6   1   0   6   0   1   2   1   0  79 409   5   0   0]\n",
      " [  2   0   1   3   0   0   0   0   0   0   0 474   5   1]\n",
      " [  1   0   3   6   0   1   0   0   0   0   0  14 473   5]\n",
      " [  7   4   7   9   1   2   0   1   1   1   1   3  17 487]]\n"
     ]
    }
   ],
   "source": [
    "#the confusion matrix\n",
    "cm = confusion_matrix(df_test['label'], NB_Result)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bda496",
   "metadata": {},
   "source": [
    "# Part III: Compare your performance to an off-the-shelf NB classifier\n",
    "Open source implementations of your custom NB classifier from Part I already exist of course. One such implementation is [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) from the sklearn library. \n",
    "\n",
    "### (5 pts) Task II: NB model comparison\n",
    "Train this model on the same data and compare its performance with your model using the metrics from part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aff01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b462fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process(content):\n",
    "    vc = CountVectorizer(input='content', stop_words = \"english\", max_features=400)\n",
    "    DTM_Count = vc.fit_transform(content)\n",
    "    ColumnNames = vc.get_feature_names()\n",
    "    DF_Count = pd.DataFrame(DTM_Count.toarray(),columns=ColumnNames)\n",
    "    return DF_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56e31e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jieyisun/opt/miniconda3/envs/TF/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train = process(df_train['content'])\n",
    "y_train = df_train['label']\n",
    "clf = MultinomialNB()\n",
    "sk_NB = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e904990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jieyisun/opt/miniconda3/envs/TF/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/jieyisun/opt/miniconda3/envs/TF/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- 1972\n",
      "- 31\n",
      "- added\n",
      "- assembly\n",
      "- campus\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 1986\n",
      "- 1987\n",
      "- 1988\n",
      "- albums\n",
      "- bank\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(len(df_test))\n",
    "test = process(df_test['content'])\n",
    "SKNB_Result = sk_NB.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec4bb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849 0.265\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "sk_acc = accuracy_score(SKNB_Result, df_test['label'])\n",
    "print(Acc, sk_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac6d4373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8503527856550249 0.2838799876012286\n"
     ]
    }
   ],
   "source": [
    "# F1 Score\n",
    "sk_f1 = f1_score(SKNB_Result, df_test['label'], average='weighted')\n",
    "print(F1, sk_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a81586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[367   6  11  11   5   9  16   6   0   1   5  10   5  13]\n",
      " [ 22 465   4   4   0   0  11   2   1   0   0   0   1   3]\n",
      " [ 17   1 306  60  20   2   3   0   1   2   2  49  16  33]\n",
      " [  5   0  12 457  11   2   0   3   0   0   0   6   0   2]\n",
      " [  6  12  17  54 360   5   3   0   3   1   5   1   0   2]\n",
      " [ 30   0   4  24   6 406   8   7   1   0   1   7  10   4]\n",
      " [ 27  15   9  16   3  14 411  12   3   3   0   1   3   5]\n",
      " [  4   0   0   3   0   3  11 449   7   3   0   0   0   0]\n",
      " [  0   3   0   0   0   0   4  10 507   0   0   0   0   0]\n",
      " [  4   1   1  14   2   0   1   6   0 372  67   0   0   1]\n",
      " [  6   1   0   6   0   1   2   1   0  79 409   5   0   0]\n",
      " [  2   0   1   3   0   0   0   0   0   0   0 474   5   1]\n",
      " [  1   0   3   6   0   1   0   0   0   0   0  14 473   5]\n",
      " [  7   4   7   9   1   2   0   1   1   1   1   3  17 487]] [[322   5  35   3   5  25  24   3   0   5   2   9  10  17]\n",
      " [ 57  98  38   1  19  64  18   1   3   5  40   0   1 168]\n",
      " [ 90   5 198   6   8  28  24   3   1   2   4  11   7 125]\n",
      " [ 46  22 169  46  16  10   3   0   3   4   4   1   9 165]\n",
      " [ 43  20  92   8  58  75  18   8   2   5   8   6  11 115]\n",
      " [150   8 124  39   6  84  12   4   0   7   4  16  13  41]\n",
      " [101  38 102   2  16  55  96  20   4  13  10  12  12  41]\n",
      " [ 43  13  42   3  11  91  19  63   0  82   4   4  45  60]\n",
      " [ 14  36   7   0  61   1 249  32  84   0   1   0  18  21]\n",
      " [ 45  49  20   3  60  16 170  19   2   7  10  15  17  36]\n",
      " [118  16  52  29  64  21  76  12   0  17   9  13  53  30]\n",
      " [ 79   0  82   6  12   4   6   1   1  15   5 257   5  13]\n",
      " [ 42   2  47   1   3   4   2   5   0   0   2   1 376  18]\n",
      " [152   4  76  12   9  34  39  18   2   2   6  17  13 157]]\n"
     ]
    }
   ],
   "source": [
    "#the confusion matrix\n",
    "sk_cm = confusion_matrix(df_test['label'], SKNB_Result)\n",
    "print(cm, sk_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2dd830-2700-4fa7-81da-057c621e9c9d",
   "metadata": {},
   "source": [
    "# Part IV: Compare NB to other classification models\n",
    "\n",
    "Now that we've built and validated our NB classifier, we want to evaluate other models on this task.\n",
    "\n",
    "### (5 pts) Task III: Evaluate the perceptron, SVM (linear), and SVM (RBF kernel)\n",
    "Train and evaluate the following models on this dataset, and compare them with the NB models.\n",
    "\n",
    "1. [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)\n",
    "2. [Linear-SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "3. [RBF-Kernel-SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "852c9e19-7a6b-43d2-86e0-245654cda5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "324363a9-ca97-4022-bdda-6dbf2b481279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849 0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jieyisun/opt/miniconda3/envs/TF/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- 1972\n",
      "- 31\n",
      "- added\n",
      "- assembly\n",
      "- campus\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 1986\n",
      "- 1987\n",
      "- 1988\n",
      "- albums\n",
      "- bank\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "# Perceptron\n",
    "per = Perceptron(tol=1e-3, random_state=0)\n",
    "per_model = per.fit(x_train, df_train['label'])\n",
    "result_per = per.predict(test)\n",
    "per_acc = accuracy_score(result_per, df_test['label'])\n",
    "#compare the accuracy of perceptron and my NB model\n",
    "print(Acc, per_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7383fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849 0.2905714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jieyisun/opt/miniconda3/envs/TF/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jieyisun/opt/miniconda3/envs/TF/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- 1972\n",
      "- 31\n",
      "- added\n",
      "- assembly\n",
      "- campus\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- 1986\n",
      "- 1987\n",
      "- 1988\n",
      "- albums\n",
      "- bank\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear-SVM\n",
    "L_SVM = LinearSVC(C=5)\n",
    "L_SVM.fit(x_train, df_train['label'])\n",
    "result_lsvm = L_SVM.predict(test)\n",
    "L_SVM_acc = accuracy_score(result_lsvm, df_test['label'])\n",
    "#compare the accuracy of Linear-SVM and my NB model\n",
    "print(Acc, L_SVM_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c9e91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF-Kernel SVM\n",
    "RBF_SVM = SVC(kernel = 'rbf', C=5, probability=True)\n",
    "RBF_SVM.fit(x_train, df_train['label'])\n",
    "result_rbfsvm = RBF_SVM.predict(test)\n",
    "RBF_acc = accuracy_score(result_rbfsvm, df_test['label'])\n",
    "#compare the accuracy of RBF-Kernel SVM and my NB model\n",
    "print(Acc, RBF_acc)\n",
    "#the result can't show up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4dea1-8ce3-4e2d-80e5-7d7ca3a1fc46",
   "metadata": {},
   "source": [
    "### (5 pts) Task IV: Select the best model\n",
    "\n",
    "1. Which model performed the best overall? \n",
    "My first Naive Bayes model performed the best overall.\n",
    "2. What metric(s) influence this decision?\n",
    "I only used the accuracy to make this decision. The accuracy of the first Naive Bayes model is much higher than other ones.\n",
    "3. Does the model that learns a non-linear decision boundary help?\n",
    "Maybe. But the result of non-linear decision boundary didn't show up even running for more than 3 hours."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('TF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "263286f8e555d11a2898a0ec2a3edf48167460b52a6319f3c1490d6e47c59551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
